% Edit by 汤
\chapter{参数估计\label{cha:6}} % p285

这里所指的参数是指如下三类未知参数：
\begin{itemize}
\item 分布中所含有未知参数 $\theta$. 如: 二点分布 $b(1,p)$ 中的 $p$; 正态分布 $N(\mu,\sigma)$ 中的 $\mu$ 和 $\sigma^2$.
\item 分布中所含的未知参数 $\theta$ 的函数. 如: 服从正态分布 $N(\mu,\sigma^2)$ 的变量 $X$ 不超过某给定值 $a$ 的概率 $P(X\leqslant a)=\Phi\big(\frac{a-\mu}{\sigma}\big)$ 是未知参数 $\mu,\sigma$ 的函数; 单位产品的缺陷数 $X$ 通常服从泊松分布 $P(\lambda)$, 则单位产品合格(无缺陷)的概率 $P(X=0)=\ee^{-\lambda}$ 是未知参数 $\lambda$ 的函数.
\item 分布的各种特征数也都是未知参数, 如: 均值 $E(X)$, 方差 $\mathrm{Var}(X)$，分布中位数等.
\end{itemize}

一般场合，常用日表示参数, 参数 $\theta$ 所有可能取值组成的集合称为参数空间，常用 $\theta$ 表示. 参数估计问题就是根据样本对上述各种未知参数作出估计. 

参数估计的形式有两种: 点估计与区间估计. 这里我们从点估计开始.

设 $x_1,x_2,\cdots,x_n$ 是来自总体的一个样本, 我们用一个统计量 $\bar{\theta}=\bar{\theta}(x_1,\cdots,x_n)$ 的取值作为 $\theta$ 的估计值, $\bar{\theta}$ 称为 $\theta$ 的点估计(量), 简称估计. 在这里如何构造统计量并没有明确的规定, 只要它满足一定的合理性即可, 这就涉及两个问题:

\begin{itemize}
\item 其一是如何给出估计, 即估计的方法问题: 
\item 其二是如何对不同的估计进行评价, 即估计的好坏判断标准. 
\end{itemize}

接下来我们先介绍一些估计的方法, 接着讨论估计的好坏标准, 然后对几个有用的专题给介绍，最后讲述区间估计. 

\section{点估计的几种方法\label{section-6-1}} %

人们可以运用各种方法构造出很多 $\theta$ 的估计, 本节介绍两种最常用的点估计方法,它们是: 矩法和最大似然法.

\subsection{替换原理和矩法估计} %6.1.1录入完毕，待检查

1900 年英国统计学家 K.Pearson 提出了一个替换原则, 后来人们称此方法为矩法. 

\subsubsection{矩法估计}
%一、矩法估计 P286

替换原理常指如下两句话:
\begin{itemize}
\item 用样本矩去替换总体矩, 这里的矩可以是原点矩也可以是中心矩; 
\item 用样本矩的函数去替换相应的总体矩的函数.
\end{itemize}

根据这个替换原理, 在总体分布形式未知场合也可对各种参数作出估计, 譬如: 
\begin{itemize}
\item 用样本均值 $\bar{x}$ 估计总体均值 $E(X)$，即 $\hat{E}(X)=\bar{x}$;
\item 用样本方差 $s_n^2$ 品估计总体方差 $\mathrm{Var}(x)$, 即 $\mathrm{\hat{V}ar}(x)=s_n^2$;
\item 用事件 $A$ 出现的频率估计事件 $A$ 发生的概率;
\item 用样本的 $p$ 分位数估计总体的 $p$ 分位数, 特别, 用样本中位数估计总体中位数.
\end{itemize}

\begin{example}
对某型号的 $20$ 辆汽车记录其每 $5L$ 汽油的行驶里程(公里), 观测数据如下: 

\begin{tabular}{cccccccccc}
29.8 & 27.6 & 28.3 & 27.9 & 30.1 & 28.7 & 29.9 & 28.0 & 27.9 & 28.7\\
28.4 & 27.2 & 29.5 & 28.5 & 28.0 & 30.0 & 29.1 & 29.8 & 29.6 & 26.9		
\end{tabular}

这是一个容量为 $20$ 的样本观测值, 对应总体是该型号汽车每 $5L$ 汽油的行驶里程, 其分布形式尚不清楚, 可用矩法估计其均值、方差和中位数等. 本例中经计算有
\[\bar{x}=28.695,\quad s_n^2=0.9185,\quad m_{0.5}=28.6,\]
由此给出总体均值、方差和中位数的估计分别为 28.695, 0.9185 和 28.6.
\end{example}

矩法估计的统计思想(替换原理)十分简单明确, 众人都能接受, 使用场合甚广它的实质是用经验分布函数去替换总体分布, 其理论基础是格里纹科定理.

\subsubsection{概率函数 $p(x;\theta)$ 已知时未知参数的矩法估计}

设总体具有已知的概率函数 $p(x;\theta_1,\cdots,\theta_k)$, $(\theta_1,\cdots,\theta_k)\in \Theta$ 未知参数或参数向量,  $x_1,\cdots,x_n$ 是样本, 假定总体的 $k$ 阶原点矩 $\mu_k$ 存在, 则对所有的 $j$, 
$0<j<k$, $A$ 内都存在, 若假设 $\theta_1,\cdots,\theta_k$. 能够表示成 $u_1,\cdots,u_k$ 的函数 $\theta_j=\theta_j(u_1,\cdots,u_k)$, 则可给出诸 $\theta_j$ 的矩法估计:
\begin{equation}
\hat{\theta}_j=\theta_j(a_1,\cdots,a_k),\quad j=1,\cdots,k,
\end{equation}
其中 $a_1,\cdots,a_k$. 是前个样本原点矩: $a_j=\frac{1}{n}\sum_{i=1}^{n}x_i^j$. 进一步, 如果我们要估计 $\theta_1,\cdots,\theta_k$
的函数 $\eta=g(\theta_1,\cdots,\theta_k)$, 则可直接得到 $\eta$ 的矩法估计
\begin{equation}
\hat{\eta}=g(\hat{\theta}_1,\cdots,\hat{\theta}_k),
\end{equation}
当 $k=1$ 时, 我们通常可以由样本均值出发对未知参数进行估计; 如果 $k=2$, 我们可以由一阶、二阶原点矩(或二阶中心矩)出发估计未知参数.

\begin{example}
设总体为指数分布, 其密度函数为
\[p(x;\lambda)=\lambda\cdot\ee^{-\lambda x},\quad x>0, \]
$x_1,\cdots,x_n$ 是样本, 此处 $k=1$, 由于 $EX=1/\lambda$, 亦即 $\lambda=1/EX$, 故 $\lambda$ 的矩法估计为
\[\hat{\lambda}=1/\bar{x}. \]
另外, 由于 $\mathrm{Var}(X)=1/\lambda^2$, 其反函数为 $\lambda=1/\sqrt{\mathrm{Var}(X)}$, 因此, 从替换原理来看, $\lambda$ 的矩法估计也可取为
\[\hat{\lambda}_1=1/s. \] 
$s$为样本标准差. 这说明矩估计可能是不唯一的, 这是矩法估计的一个缺点, 此时通常应该尽量采用低阶矩给出未知参数的估计.
\end{example}

\begin{example}
$x_1,\cdots,x_n$ 是来自 $(a,b)$ 上的均匀分布 $U(a,b)$ 的样本, $a$ 与 $b$ 均是未知参数, 这里 $k=2$, 由于
\[EX=\frac{a+b}{2},\quad\mathrm{Var}(X)=\frac{(b-a)^2}{12}, \]
不难推出
\[a=EX-\sqrt{3\mathrm{Var}(X)},\quad b=EX+\sqrt{3\mathrm{Var}(X)}, \]
由此即可得到 $a,b$ 的矩估计:
\[\hat{a}=\bar{x}-\sqrt{3}s,\quad\hat{b}=\bar{x}+\sqrt{3}s, \]
若从均匀总体 $U(a,b)$ 获得如下一个容量为 5 的样本: $4.5\quad5.0\quad4.7\quad4.0\quad4.2$, 经计算, 有 $\bar{x}=4.48,s_n=0.3962$, 于是可得 $a,b$ 的矩估计为
\[\hat{a}=4.48-0.3962\sqrt{3}=3.7938,\]
\[\hat{b}=4.48+0.3962\sqrt{3}=5.1662.\]
\end{example}

\subsection{最大似然估计}%6.1.2录入完毕，待检查

最大似然估计法是求估计用得最多的方法, 它最早是由高斯在 1821 年提出, 但一般将之归功于费希尔(R.A.Fisher), 因为费希尔在 1922 年再次提出了这种想法并证明了它的一些性质而使得最大似然法得到了广泛的应用.

为了叙述最大似然原理的直观想法, 先看两个例子.

\begin{example}
设有外形完全相同的两个箱子, 甲箱中有 99 个白球和 1 个黑球, 乙箱中有 99 个黑球和 1 个白球, 今随机地抽取一箱, 并从中随机抽取一球, 结果取得白球, 间这球是从哪一个箱子中取出？
\end{example}
\begin{solution}
不管是哪一个箱子, 从箱子中任取一球都有两个可能的结果：$A$ 表示取出白球，$B$ 表示取出黑球. 如果我们取出的是甲箱, 则 $A$ 发生的概率为 0.99, 而如果取出的是乙箱, 则 $A$ 发生的概率为 0.01. 现在一次试验中结果 $A$ 发生了, 人们的第一印象就是: “此白球 $(A)$ 最像从甲箱取出的”, 或者说, 应该认为试验条件对结果 $A$ 出现有利，从而可以推断这球是从甲箱中取出的. 这个推断很符合人们的经验事实, 这里 “最像” 就是 “最大似然” 之意.

本例中假设的数据很极端. 一般地, 我们可以这样设想: 有两个箱子中各有 100 只球，甲箱中白球的比例是 $p_1$, 乙箱中白球的比例是 $p_2$, 已知 $p_1>p_2$, 现随即地抽取一个箱子并从中抽取一球, 假定取到的是白球, 如果我们要在两个箱子中进行选择, 由子甲箱中白球的比例高于乙箱, 根据最大似然原理, 我们应该推断该球来自甲箱.
\end{solution}

\begin{example}\label{exam:6.1.5}
设产品分为合格品与不合格品两类, 我们用一个随即变量 $X$ 来表示某个产品是否合格, $X=0$ 表示合格品, $X=1$ 表示不合格品, 则 $X$ 服从二点分布 $b(1,p)$, 其中 $p$ 是未知的不合格品率. 现抽取 $n$ 个产品看其是否合格, 得到样本 $x_1,\cdots,x_n$, 这批观测值发生的概率为:
\begin{equation}\label{eq:6.1.3}
P(X_1=x_1,\cdots,X_n=x_n;p)=\prod_{i=1}^np^{x_i}(1-p)^{1-x_i}=p^{\sum x_i}(1-p)^{n-\sum x_i}, %6.1.3
\end{equation}
由子 $p$ 是未知的, 根据最大似然原理, 我们应选择 $p$ 使得 \eqref{eq:6.1.3} 表示的概率尽可能大. 将\eqref{eq:6.1.3}看作未知参数 $p$ 的函数，用 $L(p)$ 表示, 称作\textbf{似然函数}\index{G!似然函数}, 亦即
\begin{equation}\label{eq:6.1.4}
L(p)=p^{\sum x_i}(1-p)^{n-\sum x_i},
\end{equation}
要求 \eqref{eq:6.1.4} 的最大值点不是难事, 将 \eqref{eq:6.1.4} 两端取对数并关子 $p$ 求导令其为 $\Theta$, 即得如下方程:
\begin{equation}\label{eq:6.1.5}
\frac{\partial \ln L(p)}{\partial p}=\frac{\sum x_i}{p}-\frac{n-\sum x_i}{1-p}=0
\end{equation}
解之即得 $p$ 的最大\textbf{似然估计}\index{G!似然估计}, 为
\[\hat{p}=\hat{p}(x_1,\cdots,x_n)=\sum x_i/n=\bar{x}. \]
由例\ref{eq:6.1.5} 我们可以看到求最大似然估计的基本思路, 对离散型总体, 设有样本观测值 $x_1,\cdots,x_n$, 我们写出该观测值出现的概率, 它一般依赖子某个或某些参数, 用 $\theta$ 表示，将该概率看成 $\theta$ 的函数, 用 $L(\theta)$ 表示, 即
\[L(\theta)=L(X_1=x_1,\cdots,X_n=x_n;\theta), \]
求最大似然估计就是找 $\theta$ 的估计值 $\hat{\theta}=\hat{\theta}(x_1,\cdots,x_n)$ 使得上式的 $L(\theta)$ 达到最大.
\end{example}

对连续型总体, 样本观测值 $x_1,\cdots,x_n$ 出现的概率总是为 0 的, 但我们可用联合概率密度函数来表示随机变量在观测值附近出现的可能性大小, 也将之称为似然函数, 由此, 我们给出如下正规的定义.

\begin{definition}{}{}%6.1.1
设总体的概率函数为 $p(x;\theta)$, $\theta\in\Theta$, 日，其中 $\theta$ 是一个未知参数或几个未知参数组成的参数向量, 是参数 $\theta$ 可能取值的参数空间, $x_1,\cdots,x_n$ 是来自该总体的样本, 将样本的联合概率函数看成8的函数，用 $L(\theta;x_1,\cdots,x_n)$ 表示, 简记为 $L(\theta)$,
\begin{equation}\label{eq:6.1.6}
L(\theta)=L(\theta;x_1,\cdots,x_n)=p(x_1;\theta)\cdot p(x_2;\theta)\cdot \cdots \cdot p(x_n;\theta),
\end{equation}
$L(\theta)$ 称为样本的似然函数. 如果某统计量 $\hat{\theta}=\hat{\theta}(x_1,\cdots,x_n)$ 满足
\begin{equation}\label{eq:6.1.7}
L(\hat{\theta})=\max_{\theta\in\Theta}L(\theta),
\end{equation}
则称 $\hat{\theta}$ 是 $\theta$ 的\textbf{最大似然估计}\index{G!最大似然估计}, 简记为 MLE(Maximum Likelihood Estimate).
\end{definition}

由于 $\ln x$ 是 $x$ 的单调增函数, 因此, 使对数似然函数 $\ln L(\theta)$ 达到最大与使 $L(\theta)$ 达到最大是等价的. 人们通常更习惯于由 $\ln L(\theta)$ 出发寻找日的最大似然估计. 当 $L(\theta)$ 是可微函数时, 求导是求最大似然估计最常用的方法，此时对对数似然函数求导更加简单些.

\begin{example}
设一个试验有三种可能结果，其发生概率分别为
\begin{equation}\label{eq:6.1.8}
p_1=\theta^2,\quad p_2=2\theta(1-\theta),\quad p_3=(1-\theta)^2.
\end{equation}
现做了 $n$ 次试验，观测到三种结果发生的次数分别为 $n_1,n_2,n_3(n_1+n_2+n_3=n)$. 则似然函数为
\begin{align*}
L(\theta) 
&=\big(\theta^{2}\big)^{n_1}[2 \theta(1-\theta)]^{n_{2}}\left[(1-\theta)^{2}\right]^{n_{3}} \\ &=2^{n_{2}} \theta^{2 n_{1}+n_{2}}(1-\theta)^{2 n_{3}+n_{2}}
\end{align*}
其对数似然函数为
\[\ln L(\theta)=\left(2 n_{1}+n_{2}\right)\ln\theta+\left(2 n_{3}+n_{2}\right)\ln (1-\theta)+n_{2}\ln 2,\]
将之关于 $\theta$ 求导并令其为 0 得到似然方程
\[\frac{2 n_{1}+n_{2}}{\theta}-\frac{2 n_{3}+n_{2}}{1-\theta}=0,\]
解之，得
\[\hat{\theta}=\frac{2 n_{1}+n_{2}}{2\left(n_{1}+n_{2}+n_{3}\right)}=\frac{2 n_{1}+n_{2}}{2 n},\]
由于
\[\frac{\partial^{2} \ln L(\theta)}{\partial \theta^{2}}=-\frac{2 n_{1}+n_{2}}{\theta^{2}}-\frac{2 n_{3}+n_{2}}{(1-\theta)^{2}}<0,\]
所以 $\hat{\theta}$ 是极大值点.
\end{example}

\begin{example}\label{exam:6.1.7}
对正态总体 $N(u,\delta^2)$, $\theta=(u,\delta^2)$ 是二维参数, 设有样本 $x_1,\cdots,x_n$, 则似然函数及其对数分别为
\begin{align*}
L\left(\mu,\sigma^{2}\right) 
&=\prod_{i=1}^{n}\left\{\frac{1}{\sqrt{2\pi}\sigma}\exp \left\{-\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}\right\}\right\}\\ 
&=\left(2\pi\sigma^{2}\right)^{-n/2}\exp\left\{-\frac{1}{2 \sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\right\}
\end{align*}
\[\ln L\left(\mu,\sigma^{2}\right)=-\frac{1}{2\sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}-\frac{n}{2} \ln \sigma^{2}-\frac{n}{2}\ln(2\pi)\]
将 $\ln L(\mu,\delta^2)$ 分别关于两个分量求偏导并令其为 0 即得到似然方程组
将 $\ln L(u,\delta^2)$ 分别关于两个分量求偏导并令其为 0 即得到似然方程组
\begin{equation}\label{eq:6.1.9}
\frac{\partial\ln L\left(\mu, \sigma^{2}\right)}{\partial \mu}=\frac{1}{\sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-\mu\right)=0
\end{equation}
\begin{equation}\label{eq:6.1.10}
\frac{\partial \ln L\left(\mu, \sigma^{2}\right)}{\partial \sigma^{2}}=\frac{1}{2 \sigma^{4}} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}-\frac{n}{2\sigma^{2}}=0
\end{equation}
解此方程组, 由 \eqref{eq:6.1.9} 可得 $\mu$ 的最大似然估计为
\[\hat{\mu}=\frac{1}{n} \sum_{z=1}^{n} x_{i}=\overline{x},\]
将之代入 \eqref{eq:6.1.10} 给出 $\delta^2$ 的最大似然估计
\[\vec{\sigma}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}=s^{* 2},\]
利用二阶导函数矩阵的非正定性可以说明上述估计使得似然函数取极大值.

虽然求导函数是求最大似然估计最常用的方法, 但并不是在所有场合求导都是有效的, 下面的例子说明了这个问题.
\end{example}

\begin{example}%6.1.8
设 $x_1,\cdots,x_n$ 是来自均匀总体 $U(0,\theta)$ 的样本, 试求 $\theta$ 的最大似然估计.
\end{example}
\begin{solution}
似然函数
\[L(\theta)=\frac{1}{\theta^n}\prod_{i=1}^nI_{\{0<X_i\leqslant\theta\}}=\frac{1}{\theta^n}I_{\{X_{(n)}\leqslant\theta\}} \]
要使 $L(\theta)$ 达到最大, 首先一点是示性函数取值应该为 1, 其次是 $1/\theta^n$ 尽可能大. 由于 $1/\theta^n$ 是 $\theta$ 的单调减函数, 所以 $\theta$ 的取值应尽可能小, 但示性函数为 1 决定了 $\theta$ 不能小于x（a）, 由此给出 $\theta$ 的最大似然估计: $\hat{\theta}=x_{(n)}$.

最大似然估计有一个简单面有用的性质: 如果是 $\theta$ 的最大似然估计, 则对任一函数 $g(\theta)$, 其最大似然估计为 $g(\hat{\theta})$. 该性质称为最大似然估计的不变性, 从而使一些复杂结构的参数的最大似然估计的获得变得容易了.
\end{solution}

\begin{example}
设 $x_1,\cdots,x_n$ 是来自正态总体 $N(\mu,\delta^2)$ 的样本, 在例\ref{exam:6.1.7}中已求得 $\mu$ 和 $\delta^2$ 的最大似然估计为
\[\hat{\mu}=\overline{x},\quad\hat{\sigma}^{2}=s^{*2}\]
于是由最大似然估计的不变性可得如下参数的最大似然估计, 它们是
\begin{itemize}
\item 标准差 $\delta$ 的MLE是 $\delta=\sigma^*$;
\item 概率 $P(X<3)=\Phi\big(\frac{3-u}{\delta}\big)$ 的 MLE 是 $\Phi\big(\frac{3-\bar{x}}{s^*}\big)$;
\item 总体 0.90 分位数 $x_{0.90}=\mu+\sigma\cdot\mu_{0.90}$ 的 MLE 是 $\bar{x}+s^{*}\cdot u_{0.90}$, 其中 $u_{0.90}$ 为标准正态分布的 0.90 分位数.
\end{itemize}
\end{example}

\subsection{习题\label{ssec:6.1}} %p291 习题录入完毕，未检查

\begin{xiti}
\item 从一批电子元件中抽取 8 个进行寿命测试, 得到如下数据 (单位：h):
\begin{center}
\begin{tabular}{cccccccc}
	1050,&1100,&1130,&1040,&1250,&1300,&1200,&1080
\end{tabular}
\end{center}
试对这批元件的平均寿命以及寿命分布的标准差给出矩估计.
\item 设总体 $X\sim U(0,0)$, 现从该总体中抽取容量为 10 的样本, 样本值为:
\begin{center}
\begin{tabular}{cccccccccc}
0.5,&1.3,&0.6,&1.7,&2.2,&1.2,&0.8,&1.5,&2.0,&1.6
\end{tabular}	
\end{center}
试对参数 $\theta$ 给出矩估计.
\item 设总体分布列如下, $x_1,x_2,\cdots,x_n$ 是样本, 试求未知参数的矩估计.\\
(1) $P(X=k)=\frac{1}{N}, k=0,1,2, \cdots, N-1, N$, (正整数)是未知参数;\\
(2) $P(X=k)=(k-1) \theta^{2}(1-\theta)^{k-2}, k=2,3, \cdots, 0<\theta<1$.
\item 设总体密度函数如下, $x_1,x_2,\cdots,x_n$ 是样本, 试求未知参数的矩估计. \\
(1) $p(x ; \theta)=\frac{2}{\theta^{2}}(\theta-x), 0<x<\theta, \theta>0$, \\
(2) $p(x ; \theta)=(\theta+1) x^{8}, 0<x<1, \theta>0$, \\
(3) $p(x ; \theta)=\sqrt{\theta} x^{\sqrt8-t}, 0<x<1, \theta>0$, \\
(4) $p(x ; \theta, \mu)=\frac{1}{\theta} \mathrm{e}^{-\frac{x-\mu}{\theta}}, x>\mu, \theta>0$.
\item 设总体为 $N(\mu,1)$, 现对该总体观测 $n$ 次, 发现有 $k$ 次观测值为正, 使用频率替换方法求 $A$ 的估计. 
\item 甲、乙两个校对员被此独立对同一本书的样稿进行校对, 校完后, 甲发现 $a$ 个错字, 乙发现 $b$ 个错字, 其中共同发现的错字有 $c$ 个, 试用矩法给出如下两个未知参数的估计:\\
(1) 该书样稿的总错字个数;\\
(2) 未被发现的错字数.
\item 设总体概率函数如, $x_1,x_2,\cdots,x_n$ 是样本, 试求未知参数的最大似然估计. \\
(1) $p(x ; \theta)=\sqrt{\theta} x^{\sqrt{\theta}-1}, 0<x<1, \theta>0$, \\
(2) $p(x ; \theta)=\theta c^{\theta} x^{-(\theta+1)}, x>c, c>0$, 已知,  $\theta>1$.
\item 设总体概率函数如下, $x_1,x_2,\cdots,x_n$ 是样本, 试求未知参数的最大似然估计.
(1) $p(x ; \theta)=c \theta^{c} x^{-(c+1)}, x>\theta, \theta>0, c>0$, 已知 ,\\
(2) $p(x ; \theta, \mu)=\frac{1}{\theta} e^{-\frac{x-\mu}{\theta}}, x>\mu, \theta>0$ ,\\
(3) $p(x ; \theta)=(k \theta)^{-1}, \theta<x<(k+1) \theta, \theta>0$ .
\item 设总体概率函数如下, $x_1,x_2,\cdots,x_n$ 是样本, 试求未知参数的最大似然估计.\\
(1) $p(x ; \theta)=\frac{1}{2\theta} e^{-|x| / \theta}, \theta>0$ ,\\
(2) $p(x ; \theta)=1, \theta-1 / 2<x<\theta+1 / 2$ ,\\
(3) $p\left(x ; \theta_{1}, \theta_{2}\right)=\frac{1}{\theta_{2}-\theta_{1}}, \theta_{1}<x<\theta_{2}$ .
\item 一地质学家为研究密歇根湖的湖滩地区的岩石成分, 随机地自该地区取 100 个样品，每个样品有 10 块石子, 记录了每个样品中属石灰石的石子数. 假设这 100 次观豪相互独立, 求这地区石子中石灰石的比例 $p$ 的最大似然估计.该地质学家所得的数据如下: 
\begin{center}
\begin{tabularx}{0.8\textwidth}{Z|*{10}{c|}|c}
样本中的石子数 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\\hline
样品个数 & 0 & 1 & 6 & 7 & 23 & 26 & 21 & 12 & 3 & 1 & 0
\end{tabularx}
\end{center}
\item 在遗传学研究中经常要从截尾二项分布中抽样，其总体概率函数为
\[P\left(X=k; p\right)=\frac{\bigg(\begin{array}{@{}c@{}}{m} \\ {k}\end{array}\bigg) p^{k}(1-p)^{m-k}}{1-(1-p)^{m}}, k=1,2, \cdots, m\]
若已知 $m=2$, $x_1,x_2,\cdots,x_n$ 是样本，试求 $p$ 的最大似然估计.
\item 已知在文学家萧伯纳的 “An lntelligent Wotman's Guide To Socialisn”一书中, 一个句子的单词数 $X$ 近似地服从对数正态分布, 即 $x=\ln X\sim N(\mu,\sigma^2)$. 今从该书中随机地取 20 个句子, 这些句子中的单词数分别为
\begin{center}
\begin{tabularx}{0.8\textwidth}{*{10}{Z}}
52 & 24 & 15 & 67 & 15 & 22 & 63 & 26 & 16 & 32\\
7 & 33 & 28 & 14 & 7 & 29 & 10 & 6 & 59 & 30
\end{tabularx}	
\end{center}
求该书中一个句子单词数均值 $E(X)=\ee^{\mu+\sigma^2/2}$ 的最大似然估计.
\end{xiti}

\section{点估计的评价标准}\label{sec:6.2}

我们已经看到，点估计有各种不同的求法, 为了在不同的点估计间进行比较选择, 就必须对各种点估计的好坏给出评价标准.

数理统计中给出了众多的估计量评价标准，对同一估计量使用不同的评价标准可能会得到完全不同的结论, 因此, 在评价某一个估计好坏时首先要说明是在哪一个标准下, 否则所论好坏则毫无意义.

但不管怎么说, 有一个基本标准是所有的估计都应该满足的, 它是衡量一个估计是否可行的必要条件, 这就是估计的相合性, 我们就从相合性开始.

\subsection{相关性}\label{ssec:6.2.1} % P293 录入完毕，未检查

\begin{definition}{}{6.2.1}
设 $\theta\in\Theta$ 为未知参数, $\hat{\theta}_n=\hat{\theta}_n(x_1,\cdots,x_n)$ 是 $\theta$ 的一个估计量, $n$ 是样本容量, 若对任何一个 $\varepsilon>0$, 有
\begin{equation}\label{eq:6.2.1}
\lim _{n \rightarrow \infty} P(|\hat{\theta}_{n}-\theta|>\varepsilon)=0
\end{equation}
则称 $\hat{\theta}_n$ 为参数 $\theta$ 的相合估计.
\end{definition}
相合性被认为是对估计的一个最基本要求, 如果一个估计量, 在样本量不断增大时, 它都不能把被估参数估计到任意指定的精度, 那么这个估计是很值得怀疑的. 通常, 不满足相合性要求的估计一般不予考虑. 证明估计的相合性一般可应用大数定律或直接由定义来证.

若把依赖于样本量 $n$ 的估计量 $\hat{\theta}_n$. 看作一个随机变量序列, 相合性就是 $\hat{\theta}_n$. 依概率收敛于 $\theta$，所以证明估计的相合性可应用依概率收敛的性质及各种大数定律. 

\begin{example}\label{exam:6.2.1}
设 $x_1,\cdots,x_n$ 是来自正态总体 $N(\mu,\sigma^2)$ 的样本，则由辛钦大数定律及依概率收敛的性质知:
\begin{itemize}
\item $\bar x$ 是 $\mu$ 的相合估计;
\item $s^{*2}$ 是 $\sigma^2$ 的相合估计;
\item $s^2$ 也是 $\sigma^2$ 的相合估计.
\end{itemize}
由此可见参数的相合估计不止一个.
\end{example}

\begin{theorem}{}{6.2.1}
设 $\hat{\theta}_n=\hat{\theta}_n(x_1,\cdots,x_n)$ 是 $\theta$ 的一个估计量, 若
\begin{equation}\label{eq:6.2.2}
\lim _{n \rightarrow+\infty} E(\hat{\theta}_{n})=\theta, \quad \lim _{n \rightarrow+\infty} \operatorname{Var}(\hat{\theta}_{n})=0
\end{equation}
则 $\hat{\theta}_n$ 是 $\theta$ 的相合估计，
\end{theorem}\begin{proof}
对任意的 $\varepsilon>0$, 由切比雪夫不等式有
\[P(|\hat{\theta}_{n}-E \hat{\theta}_{n}| \geqslant \varepsilon / 2) \leqslant \frac{4}{\varepsilon^{2}} \operatorname{Var}(\hat{\theta}_{n})\]
另一方面，由 $\lim _{n \rightarrow+\infty} E(\hat{\theta}_{n})=\theta$ 可知, 当 $n$ 充分大时有
\[|E \hat{\theta}_{n}-\theta|<\varepsilon / 2\]
注意到此时如果 $|\hat{\theta}_n-E\hat{\theta}_n|<\varepsilon/2$, 就有
\[|\hat{\theta}_{n}-\theta| \leqslant|\hat{\theta}_{n}-E \hat{\theta}_{n}|+|E \hat{\theta}_{n}-\theta|<\varepsilon\]
故
\[\{|\hat{\theta}_{n}-E \hat{\theta}_{n}|<\varepsilon / 2\} \subset| | \hat{\theta}_{n}-\theta |<\varepsilon \}\]
等价地
\[\{ | \hat{\theta}_{n}-E \hat{\theta}_{n}|< \varepsilon/ 2 \} \supset\{|\hat{\theta}_{n}-\theta|<\varepsilon \}\]
由此即有
\[P(|\hat{\theta}_{n}-\theta|>\epsilon) \leqslant P(|\hat{\theta}_{n}-E \hat{\theta}_{n}| \geqslant \epsilon / 2) \leqslant \frac{4}{\varepsilon^{2}} \operatorname{Var}(\hat{\theta}_{n}) \rightarrow 0(n \rightarrow+\infty)\]
定理得证.
\end{proof}

\begin{example}\label{exam:6.2.2}
设 $x_1,\cdots,x_n$ 是来自均匀总体 $U(0,0)$ 的样本, 证明 $\theta$ 的最大似然估计是相合估计.
\end{example}
\begin{proof}
在例\ref{exam:6.1.7}中我们已经给出 $\theta$ 的最大似然估计是 $x_{(n)}$.  由次序统计量的分布, 我们知道 $\hat{\theta}=x_{(n)}$ 的分布密度函数为
\[p(y)=n y^{n-1} / \theta^{n}, \quad y<\theta\]
故有
\[E \hat{\theta}=\int_{0}^{\theta} n y^{n}\dd y / \theta^{n}=\frac{n}{n+1} \theta \rightarrow \theta\]
\[E \hat{\theta^{2}}=\int_{0}^{\theta} n y^{n+1} \mathrm{d} y / \theta^{n}=\frac{n}{n+2} \theta^{2}\]
\[\operatorname{Var}(\hat{\theta})=\frac{n}{n+2} \theta^{2}-\left(\frac{n}{n+1} \theta\right)^{2}=\frac{n}{(n+1)^{2}(n+2)} \theta^{2} \rightarrow 0 \quad(n \rightarrow+\infty)\]
由定理\ref{thm:6.2.1}可知,  $x_{(n)}$ 是 $\theta$ 的相合估计.
\end{proof}

\begin{theorem}{}{6.2.2} %定理6.2.2 P294
若 $\hat{\theta}_{n1},\cdots,\hat{\theta}_{nk}$ 分别是 $\theta_1,\cdots,\theta_k$ 的相合估计, $\eta=g(\theta_1,\cdots,\theta_k)$ 是 $\theta_1,\cdots,\theta_k$ 的连续函数，则 $\bar{\eta}_n=g(\hat{\theta}_{n1},\cdots,\hat{\theta}_{nk})$ 是 $\eta$ 的相合估计.
\end{theorem}
\begin{proof}
由函数 $g$ 的连续性, 对任意给定的 $\varepsilon>0$, 存在一个 $\delta>0$, 当 $|\hat{\theta}_j-\theta_j|<\delta,j=1,\cdots,k$, 有
\begin{equation}\label{eq:6.2.3}
|g(\hat{\theta}_{1}, \cdots, \hat{\theta}_{k})-g(\theta_{1}, \cdots, \theta_{k})|<\varepsilon
\end{equation}
又由 $\hat{\theta}_{n1},\cdots,\hat{\theta}_{nk}$ 是的相合性, 对给定的 $\delta$, 对任意给定的 $v>0$, 存在正整数 $N$, 使得 $n\geqslant N$ 时, 
\[P(|\hat{\theta}_{n j}-\theta_{j}| \geqslant \delta)<v / k, \quad j=1, \cdots, k\]
从而有
\begin{align*}
P\left(\bigcap_{i=1}^{k}\{|\hat{\theta}_{n j}-\theta_{j}|<\delta\}\right)
&=1-P\left(\bigcup_{j=1}^{k}\{ | \vec{\theta}_{n j}-\theta_{j} | \geqslant \delta\}\right)\\
&\geqslant 1-\sum_{j=1}^{k} P(|\hat{\theta}_{n j}-\theta_{j}| \geqslant \delta)\\
&>1-k \cdot v / k=1-v
\end{align*}
根据\eqref{eq:6.2.3}, $\bigcap_{j=1}^{k}\{|\hat{\theta}_{n j}-\theta_{j}|<\delta\} \subset\{ | \hat{\eta}_{n}-\eta |<\varepsilon \}$ 故有
\[P\left(\left|\hat{\eta}_{n}-\eta\right|<\epsilon\right)>1-v\]
由v的任意性，定理得证.
\end{proof}

由大数定律及定理\ref{thm:6.2.2}, 我们可以看到, 矩估计一般都具有相合性. 比如: 
\begin{itemize}
\item 样本均值是总体均值的相合估计;
\item 样本标准差是总体标准差的相合估计;
\item 样本变异系数 $s/\bar x$ 是总体变异系数的相合估计.
\end{itemize}

\begin{example}\label{exam:6.2.3}
设一个试验有三种可能结果, 其发生概率分别为
\[p_{1}=\theta^{2}, \quad p_{2}=2 \theta(1-\theta), p_{3}=(1-\theta)^{2}\]
现做了 $n$ 次试验，观测到三种结果发生的次数分别为 $n_1,n_2,n_3$ 可以采用频率替换方法估计 $\theta$. 由于可以有三个不同的日的表达式：
\[\theta=\sqrt{p_{1}}, \quad \theta=1-\sqrt{p_{3}}, \quad \theta=p_{1}+p_{2} / 2\]
从而可以给出 $\theta$ 三种不同的频率替换估计，它们分别是：
\[\hat{\theta}_{1}=\sqrt{n_{1} / n}, \quad \hat{\theta}_{2}=1-\sqrt{n_{3} / n}, \quad \hat{\theta}_{3}=\left(n_{1}+n_{2} / 2\right) / n\]
由大数定律, $n_1/n,n_2/n,n_3/n$ 分别是 $p_1,p_2,p_3$ 的相合估计，由定理\ref{thm:6.2.2}知, 上述三个估计都是 $\theta$ 的相合估计.
\end{example}

\subsection{无偏性}\label{ssec:6.2.2} % P295 录入完毕，未检查

相合性是大样本下估计量的评价标准, 对小样本而言, 需要一些其他的评价标准, 无偏性便是一个常用的评价标准.

\begin{definition}{}{6.2.2} % P295
设 $\hat{\theta}_n=\hat{\theta}_n(x_1,\cdots,x_n)$ 是 $\theta$ 的一个估计, $\theta$ 的参数空间为 $\Theta$ ，若对任意的 $\theta\in\Theta$, 有
\begin{equation}\label{eq:6.2.4}
E(\hat{\theta})=\theta
\end{equation}
则称 $\hat\theta$ 是 $\theta$ 的{\heiti 无偏估计}\index{G!无偏估计}, 否则称为{\heiti 有偏估计}\index{G!有偏估计}.
\end{definition}

无偏性要求可以改写为 $E(\hat{\theta}-\theta)=0$, 这表示无偏估计没有系统偏差，当我们使用 $\hat{\theta}$ 估计 $\theta$ 时, 由于样本的随机性, $\hat{\theta}$ 与 $\theta$ 总是有偏差的, 这种偏差时而（对某些样本观测值）为正, 时而(对另一些样本观测值)为负, 时而大, 时而小. 无偏性表示, 把这些偏差平均起来其值为 0, 这就是无偏估计的含义. 而若估计不具有无偏性, 则无论使用多少次, 其平均也会与参数真值有一定的距离, 这个距离就是系统误差.

\begin{example}\label{exam:6.2.4}
对任一总体而言, 样本均值是总体均值的无偏估计. 当总体 $k$ 阶矩存在时, 样本 $k$ 阶原点矩 $a_k$ 是总体息 $k$ 阶原点矩 $\mu_k$ 么的无偏估计. 但对 $k$ 阶中心矩则不一样, 譬如, 样本方差 $s^{*2}$ 就不是总体方差 $\sigma^2$ 的无偏估计, 因在定理\ref{thm:5.2.1}
中已经指出:
\[E\left(s^{* 2}\right)=\frac{n-1}{n} \sigma^{2}\]
对此, 有如下两点说明：

(1) 当样本量趋于无穷时, 有 $E(s^{*2})\to\sigma^2$, 我们称 $s^{*2}$ 为 $\sigma^2$ 的渐近无偏估计, 这表明当样本量较大时,  $s^{*2}$ 可近似看作 $\sigma^2$ 的无偏估计.

(2) 若对 $s^{*2}$ 作如下修正:
\begin{equation}\label{eq:6.2.5}
s^{2}=\frac{n s^{* 2}}{n-1}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\overline{x}\right)^{2}
\end{equation}
则 $s^2$ 是总体方差的无偏估计.这种简单的修正方法在一些场合常被采用.\eqref{eq:6.2.5}定义的 $s^2$ 也称为样本方差，它比 $s^{*2}$ 更常用.这是因为在 $n\geqslant2$ 时，$s^{*2}<s^2$，因此用 $s^{*2}$ 估计 $\sigma^2$ 有偏小的倾向，特别在小样本场合要使用 $s^2$ 估计 $\sigma^2$.

无偏性不具有不变性. 即若 $\hat{\theta}$ 是 $\theta$ 的无偏估计, 一般而言, $g(\hat{\theta})$ 不是 $g(\theta)$ 的无偏估计, 除非 $g(\theta)$ 是 $\theta$ 的线性函数.譬如，$s^2$ 是 $\sigma^2$ 的无偏估计, 但 $s$ 不是 $\sigma$ 的无偏估计. 下而我们以正态分布为例加以说明.
\end{example}

\begin{example}\label{exam:6.2.5}
设总体为 $N(\mu,\sigma^{2}), x_{1}, \cdots, x_{n}$ 是样本，我们已经指出 $s^2$ 是 $\sigma^2$ 的无偏估计. 由定理\ref{thm:5.3.1}, $Y=\frac{(n-1) s^{2}}{\sigma^{2}} \sim \chi^{2}(n-1)$, 其密度函数为
\[p(y)=\frac{1}{2^{\frac{n-1}{2}} \Gamma(\frac{n-1}{2})} y^{\frac{n-1}{2}}\ee^{-\frac{y}{2}}, \quad y>0\]
从而
\begin{align*}
E(Y^{1/2}) 
&=\int_{0}^{+\infty} y^{1 / 2} p(y)\dd y \\ &=\frac{1}{2^{\frac{n-1}{2}} \Gamma(\frac{n-1}{2})} \int_{0}^{\infty} y^{\frac{n}{2}-1} \mathrm{e}^{-\frac{y}{2}} \dd y \\
&=\frac{2^{\frac{n}{2}} \Gamma(\frac{n}{2})}{2^{\frac{n-1}{2}} \Gamma(\frac{n-1}{2})}=\sqrt{2} \frac{\Gamma(\frac{n}{2})}{\Gamma(\frac{n-1}{2})}
\end{align*}
由此, 我们有
\[E s=\frac{\sigma}{\sqrt{n-1}} E(Y^{1 / 2})=\sqrt{\frac{2}{n-1}} \cdot \frac{\Gamma(n / 2)}{\Gamma((n-1) / 2)} \cdot \sigma \equiv \frac{\sigma}{c_{n}}\]
这说明 $s$ 不是 $\sigma$ 的无偏估计，利用修正技术可得 $c_n\cdot s$ 是 $\sigma$ 的无偏估计，其中 $c_n=\sqrt{\frac{n-1}{2}} \cdot \frac{\Gamma((n-1) / 2)}{\Gamma(n / 2)}$ 是修偏系数，表\ref{tab:6.2.1}给出了 $c_n$ 的部分取值. 可以证明, 当 $n\to+\infty$ 时有 $c_n\to1$, 这说明 $s$ 是 $\sigma$ 的渐近无偏估计, 从而在样本容量较大时, 不经修正的 $s$ 也是 $\sigma$ 的一个很好的估计.
\begin{table}[htbp]
\centering
\caption{正态标准差的修信系数表}\label{tab:6.2.1}
\begin{tabular}{>{$}c<{$}*{4}{>{$}c<{$}||>{$}c<{$}}>{$}c<{$}}
\toprule
n &   c_n  &  n &   c_n  &  n &   c_n  &  n &   c_n  &  n & c_n \\
\midrule
  &        &  7 & 1.0424 & 13 & 1.0210 & 19 & 1.0140 & 25 & 1.0105\\
2 & 1.2533 &  8 & 1.0362 & 14 & 1.0194 & 20 & 1.0132 & 26 & 1.0100\\
3 & 1.1284 &  9 & 1.0317 & 15 & 1.0180 & 21 & 1.0126 & 27 & 1.0097\\
4 & 1.0854 & 10 & 1.0281 & 16 & 1.0168 & 22 & 1.0120 & 28 & 1.0093\\
5 & 1.0638 & 11 & 1.0253 & 17 & 1.0157 & 23 & 1.0114 & 29 & 1.0090\\
6 & 1.0509 & 12 & 1.0230 & 18 & 1.0148 & 24 & 1.0109 & 30 & 1.0087\\
\bottomrule
\end{tabular}
\end{table}
\end{example}

\subsection{有效性}

\subsection{有效性}\label{ssec:6.2.3}

参数的无偏估计可以有很多, 如何在无偏估计中进行选择? 直观的想法是希望该估计围绕参数真值的波动越小越好, 波动大小可以用方差来衡量, 因此人们常用无偏估计的方差的大小作为度量无偏估计优劣的标准, 这就是有效性.

\begin{definition}{}{6.2.3} %p297
设 $\hat{\theta}_1,\hat{\theta}_2$ 是 $\theta$ 的两个无偏估计, 如果对任意的 $\theta\in\Theta$ 有
\[\mathrm{Var}(\hat{\theta}_1)\leqslant\mathrm{Var}(\hat{\theta}_2)\]
且至少有一个 $\theta\in\Theta$ 日使得上述不等号严格成立, 则称 $\hat{\theta}_1$ 比 $\hat{\theta}_2$ 有效.
\end{definition}

\begin{example}\label{exam:6.2.6}
设 $x_1,\cdots,x_n$ 是取自某总体的样本, 记总体均值为 $\mu$, 总体方差为 $\sigma^2$, 则 $\hat{\mu}_1=x_1,\hat{\mu}_2=\hat{x}$ 都是 $\mu$ 的无偏估计，但
\[\operatorname{Var}\left(\hat{\mu}_{1}\right)=\sigma^{2}, \quad \operatorname{Var}\left(\hat{\mu}_{2}\right)=\sigma^{2} / n\]
显然, 只要 $n>1$, $\hat{\mu}_2$ 比$\hat{\mu}_1$ 有效. 这表明, 用全部数据的平均估计总体均值要比只使用部分数据更有效.
\end{example}

\begin{example}\label{exam:6.2.7}
在例\ref{exam:6.2.2}中, 我们指出均匀总体 $U(0,\theta)$ 中 $\theta$ 的极大似然估计是 $x_{(n)}$, 由于 $E_{x_{(n)}}=\frac{n}{n+1}\theta$, 所以 $x_{(n)}$ 不是 $\theta$ 的无偏估计, 但是 $\theta$ 的渐近无偏估计. 经过修偏后可以得到 $\theta$ 的一个无偏估计: $\hat{\theta}_1=\frac{n+1}{n}x_{(n)}$. 且
\begin{align*} 
\operatorname{Var}\left(\hat{\theta}_{1}\right) 
&=\left(\frac{n+1}{n}\right)^{2} \operatorname{Var}(x_{(n)}) \\ 
&=\left(\frac{n+1}{n}\right)^{2} \frac{n}{(n+1)^{2}(n+2)} \theta^{2}=\frac{\theta^{2}}{n(n+2)}
\end{align*}
另一方面, 由矩法, 我们可以得到 $\theta$ 的另一个无偏估计 $\hat{\theta}_2=2\bar x$，且
\[\operatorname{Var}(\hat{\theta}_{2})=4 \mathrm{V} \operatorname{ar}(\overline{x})=\frac{4}{n} \operatorname{Var}(X)=\frac{4}{n} \cdot \frac{\theta^{2}}{12}=\frac{\theta^{2}}{3 n}\]
由此, 当 $n>1$ 时, $\hat{\mu}_1$ 比$\hat{\mu}_2$ 有效.
\end{example}

\subsection{均方误差}

无偏性是估计的一个优良性质, 对无偏估计我们还可以通过其方差进行有效性比较. 然而不能由此认为: 有偏估计一定是不好的估计.

在有些场合，有偏估计比无偏估计更优, 这就涉及如何对有偏估计进行评价. 一般而言, 在样本量一定时, 评价一个点估计的好坏使用的度最指标总是点估计值 $\hat{\theta}$ 与参数真值 $\theta$ 的距离的函数, 最常用的函数是距离的平方.由于具有随机性, 可以对该函数求期望, 这就是下式给出的均方误差
\begin{equation}\label{eq:6.2.6}
\operatorname{MSE}(\hat{\theta})=E(\hat{\theta}-\theta)^{2}
\end{equation}
均方误差是评价点估计的最一般的标准. 自然, 我们希望估计的均方误差越小越好.

注意到
\begin{align*} 
\operatorname{MSE}(\hat{\theta}) 
&=[E(\hat{\theta}-E \hat{\theta})+(E \hat{\theta}-\theta)]^{2} \\ &=E(\hat{\theta}-E \hat{\theta})^{2}+(E \hat{\theta}-\theta)^{2}+2 E[(\hat{\theta}-E \hat{\theta})(E \hat{\theta}-\theta)] \\ &=\operatorname{Var}(\hat{\theta})+(E \hat{\theta}-\theta)^{2} \end{align*}
因此, 均方误差由点估计的方差与偏差的平方两部分组成. 如果 $\hat{\theta}$ 是 $\theta$ 的无偏估计, 则 $\operatorname{MSE}(\hat{\theta})=\operatorname{Var}(\hat{\theta})$, 此时用均方误差评价点估计与用方差是完全一样的, 这也说明了用方差考察无偏估计有效性是合理的. 当 $\hat{\theta}$ 不是 $\theta$ 的无偏估计时, 就要看其均方误差 $\operatorname{MSE}(\hat{\theta})$, 即不仅要看其方差大小, 还要看其偏差大小. 下面的例子说明在均方误差的含义下有些有偏估计优于无偏估计. 

\begin{example}
在例\ref{exam:6.2.7}中我们指出对均匀总体 $U(0,\theta)$, 由 $\theta$ 的最大似然估计得到的无偏估计是 $\hat{\theta}=(n+1)x_{(n)}/n$, 它的均方误差
\[\operatorname{MSE}(\hat{\theta})=\operatorname{Var}(\tilde{\theta})=\frac{\theta^{2}}{n(n+2)}\]
现我们考虑 $\theta$ 的形如 $\theta_\alpha=\alpha\cdot x_{(n)}$ 的估计, 其均方误差为
\begin{align*}
\operatorname{MSE}(\hat{\theta}_{\alpha}) 
&=\operatorname{Var}(\alpha \cdot x_{(n)})+(\alpha E x_{\langle n)}-\theta)^{2} \\ 
&=\alpha^{2} \operatorname{Var}(x_{(n)})+\left(\alpha \frac{n}{n+1} \theta-\theta\right)^{2} \\ 
&=\alpha^{2} \frac{n}{(n+1)^{2}(n+2)} \theta^{2}+\left(\frac{n \cdot \alpha}{n+1}-1\right)^{2} \theta^{2} 
\end{align*}
用求导的方法不难求出当 $\alpha_{0}=(n+2) /(n+1)$ 时上述均方误差达到最小, 且 $\operatorname{MSE}\big(\frac{n+2}{n+1} x_{(n)}\big)=\frac{\theta^{2}}{(n+1)^{2}}$, 这表明 $\hat{\theta}_{0}=\frac{n+2}{n+1} x_{(n)}$ 虽是 $\theta$ 的有偏估计, 但其均方课差 $\operatorname{MSE}(\hat{\theta}_{0})=\frac{\theta^{2}}{(n+1)^{2}}<\frac{\theta^{2}}{n(n+2)}=\operatorname{MSE}(\hat{\theta})$. 所以在均方课差的标准下, 有偏估计 $\hat\theta$ 优于无偏估计 $\theta$.
\end{example}

\subsection{习题}

\section{最小方差无偏估计}
\subsection{Rao-Blackwell定理}
\subsection{最小方差无偏估计}
\subsection{Cramer-Rao不等式}
\subsection{习题}

\section{贝叶斯统计}
\subsection{统计推断的基础}
\subsection{贝叶斯公式的密度函数形式}
\subsection{贝叶斯估计}
\subsection{共轭先验分部}
\subsection{习题}

\section{区间估计}
\subsection{区间估计的概念}
\subsection{枢轴量法}
\subsection{单个正态总体参数的置信区间}
\subsection{大样本置信区间}
\subsection{两个正态总体下的置信区间}
\subsection{习题}
